# Default values for TBMQ.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Configures authentication for pulling images from a private Docker registry.
# By default, TBMQ images are publicly available, but authentication settings can be provided if using a private registry.
dockerAuth:
  # Docker registry for TBMQ images.
  registry: https://index.docker.io/v1/
  # Docker username on which pull secret will be created.
  username: ""
  # Docker user password on which pull secret will be created.
  password: ""

# Controls the installation options, including database schema initialization and ArgoCD support.
installation:
  # This field is responsible for the installation process of TBMQ PostgreSQL database schema.
  installDbSchema: false
  # When this chart installing via ArgoCd.
  argocd: false

# Section to configure TBMQ service.
tbmq:
  # Docker image configuration.
  image:
    # Docker image repository for TBMQ node.
    repository: thingsboard/tbmq-node
    # Image tag/version.
    tag: 2.1.0
  # Kubernetes secret for pulling private images.
  imagePullSecret: regcred
  # Image pull policy.
  imagePullPolicy: Always
  # Statefulset option for TBMQ service.
  statefulSet:
    # Replica count for TBMQ services.
    replicas: 2
    # Custom annotations applied to the StatefulSet resource (metadata.annotations).
    # These are useful for CI/CD tools, Helm diff, audit tracking, etc.
    annotations: {}
  # List of ports for TBMQ service.
  ports:
    - name: http
      value: 8083
    - name: https
      value: 443
    - name: mqtt
      value: 1883
    - name: mqtts
      value: 8883
    - name: mqtt-ws
      value: 8084
    - name: mqtt-wss
      value: 8085
  # Node selector for choosing nodes for scheduling pods.
  # Note that this is a default kubernetes object, so you need to specify a whole object, not just labels.
  # Usually it is Map type. Example:
  # nodeSelector:
  #    role: tbmq-node
  # See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/ for more details.
  nodeSelector: {}
  # Affinity for choosing nodes for scheduling pods.
  # Note that this is a default kubernetes object, so you need to specify a whole object, not just labels.
  # Example:
  # affinity:
  #    nodeAffinity:
  #      requiredDuringSchedulingIgnoredDuringExecution:
  #        nodeSelectorTerms:
  #        - matchExpressions:
  #          - key: topology.kubernetes.io/zone
  #            operator: In
  #            values:
  #            - antarctica-east1
  #            - antarctica-west1
  # See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/ for more details.
  affinity: {}
  # Custom environment variables that are always applied, regardless of configuration source.
  # These variables will be appended to the container environment and will override any conflicting
  # variables set in `existingConfigMap` or `existingJavaOptsConfigMap`.
  #
  # Example usage:
  #
  # customEnv:
  #   SECURITY_MQTT_SSL_ENABLED: "true"
  #
  # Even if `existingConfigMap` or `existingJavaOptsConfigMap` define `SECURITY_MQTT_SSL_ENABLED`,
  # the value from `customEnv` will take precedence.
  customEnv: { SECURITY_MQTT_BASIC_ENABLED: "true" }
  # Name of an existing ConfigMap that will override TBMQ Java and Logback configurations.
  # If set, this ConfigMap should contain BOTH Java options (`conf` key) and Logback settings (`logback` key).
  #
  # This has the highest priority and will ignore `existingJavaOptsConfigMap` and `existingLogbackConfigMap`
  # but does NOT override environment variables set via `customEnv` or `envFrom` sources (e.g., PostgreSQL, Kafka, Redis configurations).
  #
  # If not set, TBMQ will fall back to `existingJavaOptsConfigMap` and `existingLogbackConfigMap`.
  # If none are provided, default configurations will be used.
  #
  # For examples of the expected structure, see `existingJavaOptsConfigMap` and `existingLogbackConfigMap`.
  existingConfigMap: ""
  # Name of an existing TBMQ Java options config map.
  # This ConfigMap should contain a key named `conf` with Java options.
  #
  # Custom ConfigMap example:
  #
  # apiVersion: v1
  # kind: ConfigMap
  # metadata:
  #   name: my-tbmq-java-opts-config
  # data:
  #   conf: |
  #     export JAVA_OPTS="$JAVA_OPTS -Dplatform=deb -Dinstall.data_dir=/usr/share/thingsboard-mqtt-broker/data"
  #     export JAVA_OPTS="$JAVA_OPTS -Xlog:gc*,heap*,age*,safepoint=debug:file=/var/log/thingsboard-mqtt-broker/${TB_SERVICE_ID}-gc.log:time,uptime,level,tags:filecount=10,filesize=10M"
  #     ...
  #     export LOG_FILENAME=thingsboard-mqtt-broker.out
  #     export LOADER_PATH=/usr/share/thingsboard-mqtt-broker/conf"
  #     ...
  #
  # The following file contains an example of values that should be placed under the `conf` key:
  # https://github.com/thingsboard/tbmq/blob/main/docker/tb-mqtt-broker/conf/thingsboard-mqtt-broker.conf
  existingJavaOptsConfigMap: ""
  # Name of an existing TBMQ logback config map.
  # This ConfigMap should contain a key named `logback` with the logging configuration.
  #
  # Custom ConfigMap example:
  #
  # apiVersion: v1
  # kind: ConfigMap
  # metadata:
  #   name: my-tbmq-logging-config
  # data:
  #   logback: |
  #     <!DOCTYPE configuration>
  #     <configuration scan="true" scanPeriod="10 seconds">
  #         <appender name="fileLogAppender"
  #                   class="ch.qos.logback.core.rolling.RollingFileAppender">
  #             <file>/var/log/thingsboard-mqtt-broker/${TB_SERVICE_ID}/thingsboard-mqtt-broker.log</file>
  #             <rollingPolicy
  #                     class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
  #                 <fileNamePattern>/var/log/thingsboard-mqtt-broker/${TB_SERVICE_ID}/thingsboard-mqtt-broker.%d{yyyy-MM-dd}.%i.log</fileNamePattern>
  #                 <maxFileSize>100MB</maxFileSize>
  #                 <maxHistory>30</maxHistory>
  #                 <totalSizeCap>3GB</totalSizeCap>
  #             </rollingPolicy>
  #         </appender>
  #         ...
  #
  # The following file contains an example of values that should be placed under the `logback` key:
  # https://github.com/thingsboard/tbmq/blob/main/docker/tb-mqtt-broker/conf/logback.xml
  existingLogbackConfigMap: ""
  # Set this parameter to false to disable automatic pod restarts on configMap or secrets updates.
  enableChecksumAnnotations: true
  # Custom annotations applied to the TBMQ pods (spec.template.metadata.annotations).
  # These are commonly used for service discovery (e.g., Prometheus scraping) or logging agents.
  annotations: {}
  # Default readiness probe for TBMQ service.
  readinessProbe:
    tcpSocket:
      # Port checked to determine readiness.
      port: 1883
    # Maximum time to wait before considering the check failed.
    timeoutSeconds: 10
    # Delay before the first readiness probe.
    initialDelaySeconds: 30
    # Interval between probe executions.
    periodSeconds: 20
    # Minimum number of successes before marking pod as "ready".
    successThreshold: 1
    # Number of failures before the pod is removed from service endpoints.
    failureThreshold: 5
  # Default liveness probe for TBMQ service.
  livenessProbe:
    tcpSocket:
      # Port checked to determine liveness.
      port: 1883
    # Maximum time to wait before considering the check failed.
    timeoutSeconds: 10
    # Delay before the first liveness probe.
    initialDelaySeconds: 60
    # Interval between probe executions.
    periodSeconds: 10
    # Minimum number of successes before marking pod as "alive".
    successThreshold: 1
    # Number of consecutive failures before restarting the pod.
    failureThreshold: 10
  # Defines the restart policy for TBMQ pods.
  restartPolicy: Always
  securityContext:
    # User ID to run the container.
    runAsUser: 799
    # Enforces non-root execution.
    runAsNonRoot: true
    # File system group ID for permissions.
    fsGroup: 799
  # Defines CPU/memory requests & limits.
  # Example:
  # resources:
  #   limits:
  #     cpu: "1.5"
  #     memory: 3Gi
  #   # Default requests for CPU and Memory that pods will try to use.
  #   requests:
  #     cpu: "1.5"
  #     memory: 3Gi
  # See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#pod-level-resource-specification for more details.
  resources: {}

## Section to configure TBMQ Integration executor service.
tbmq-ie:
  # Docker image configuration.
  image:
    # Docker image repository for TBMQ-IE node.
    repository: thingsboard/tbmq-integration-executor
    # Image tag/version.
    tag: 2.1.0
  # Kubernetes secret for pulling private images.
  imagePullSecret: regcred
  # Image pull policy.
  imagePullPolicy: Always
  # Statefulset option for TBMQ-IE service.
  statefulSet:
    # Replica count for TBMQ-IE services.
    replicas: 2
    # Custom annotations applied to the StatefulSet resource (metadata.annotations).
    # These are useful for CI/CD tools, Helm diff, audit tracking, etc.
    annotations: {}
  # List of ports for TBMQ-IE service.
  ports:
    - name: http
      value: 8082
  # Node selector for choosing nodes for scheduling pods.
  # Note that this is a default kubernetes object, so you need to specify a whole object, not just labels.
  # Usually it is Map type. Example:
  # nodeSelector:
  #    role: tbmq-ie
  # See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/ for more details.
  nodeSelector: {}
  # Affinity for choosing nodes for scheduling pods.
  # Note that this is a default kubernetes object, so you need to specify a whole object, not just labels.
  # Example:
  # affinity:
  #    nodeAffinity:
  #      requiredDuringSchedulingIgnoredDuringExecution:
  #        nodeSelectorTerms:
  #        - matchExpressions:
  #          - key: topology.kubernetes.io/zone
  #            operator: In
  #            values:
  #            - antarctica-east1
  #            - antarctica-west1
  # See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/ for more details.
  affinity: {}
  # Custom environment variables that are always applied, regardless of configuration source.
  # These variables will be appended to the container environment and will override any conflicting
  # variables set in `existingConfigMap` or `existingJavaOptsConfigMap`.
  #
  # Example usage:
  #
  # customEnv:
  #   TB_SERVICE_INTEGRATIONS_EXCLUDED: "HTTP"
  #
  # Even if `existingConfigMap` or `existingJavaOptsConfigMap` define `TB_SERVICE_INTEGRATIONS_EXCLUDED`,
  # the value from `customEnv` will take precedence.
  customEnv: {}
  # Name of an existing ConfigMap that will override TBMQ-IE Java and Logback configurations.
  # If set, this ConfigMap should contain BOTH Java options (`conf` key) and Logback settings (`logback` key).
  #
  # This has the highest priority and will ignore `existingJavaOptsConfigMap` and `existingLogbackConfigMap`
  # but does NOT override environment variables set via `customEnv` or `envFrom` sources (e.g., PostgreSQL, Kafka, Redis configurations).
  #
  # If not set, TBMQ will fall back to `existingJavaOptsConfigMap` and `existingLogbackConfigMap`.
  # If none are provided, default configurations will be used.
  #
  # For examples of the expected structure, see `existingJavaOptsConfigMap` and `existingLogbackConfigMap`.
  existingConfigMap: ""
  # Name of an existing TBMQ-IE Java options config map.
  # This ConfigMap should contain a key named `conf` with Java options.
  #
  # Custom ConfigMap example:
  #
  # apiVersion: v1
  # kind: ConfigMap
  # metadata:
  #   name: my-tbmq-ie-java-opts-config
  # data:
  #   conf: |
  #     export JAVA_OPTS="$JAVA_OPTS -Xlog:gc*,heap*,age*,safepoint=debug:file=/var/log/tbmq-integration-executor/${TB_SERVICE_ID}-gc.log:time,uptime,level,tags:filecount=10,filesize=10M"
  #     export JAVA_OPTS="$JAVA_OPTS -XX:+IgnoreUnrecognizedVMOptions -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/var/log/tbmq-integration-executor/${TB_SERVICE_ID}-heapdump.bin"
  #     ...
  #     export LOG_FILENAME=tbmq-integration-executor.out
  #     export LOADER_PATH=/usr/share/tbmq-integration-executor/conf
  #     ...
  #
  # The following file contains an example of values that should be placed under the `conf` key:
  # https://github.com/thingsboard/tbmq/blob/main/docker/tbmq-integration-executor/conf/tbmq-integration-executor.conf
  existingJavaOptsConfigMap: ""
  # Name of an existing TBMQ-IE logback config map.
  # This ConfigMap should contain a key named `logback` with the logging configuration.
  #
  # Custom ConfigMap example:
  #
  # apiVersion: v1
  # kind: ConfigMap
  # metadata:
  #   name: my-tbmq-logging-config
  # data:
  #   logback: |
  #     <!DOCTYPE configuration>
  #     <configuration scan="true" scanPeriod="10 seconds">
  #         <appender name="fileLogAppender"
  #                   class="ch.qos.logback.core.rolling.RollingFileAppender">
  #             <file>/var/log/thingsboard-mqtt-broker/${TB_SERVICE_ID}/thingsboard-mqtt-broker.log</file>
  #             <rollingPolicy
  #                     class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
  #                 <fileNamePattern>/var/log/thingsboard-mqtt-broker/${TB_SERVICE_ID}/thingsboard-mqtt-broker.%d{yyyy-MM-dd}.%i.log</fileNamePattern>
  #                 <maxFileSize>100MB</maxFileSize>
  #                 <maxHistory>30</maxHistory>
  #                 <totalSizeCap>3GB</totalSizeCap>
  #             </rollingPolicy>
  #         </appender>
  #         ...
  #
  # The following file contains an example of values that should be placed under the `logback` key:
  # https://github.com/thingsboard/tbmq/blob/main/docker/tbmq-integration-executor/conf/logback.xml
  existingLogbackConfigMap: ""
  # Controls whether Helm automatically restarts TBMQ pods when ConfigMaps or Secrets change.
  enableChecksumAnnotations: true
  # Custom annotations applied to the TBMQ-IE pods (spec.template.metadata.annotations).
  # These are commonly used for service discovery (e.g., Prometheus scraping) or logging agents.
  annotations: {}
  # Default readiness probe for TBMQ-IE service.
  readinessProbe:
    # Interval between probe executions.
    periodSeconds: 20
    tcpSocket:
      # Port checked to determine readiness.
      port: http
  # Default liveness probe for TBMQ-IE service.
  livenessProbe:
    # Delay before the first liveness probe.
    initialDelaySeconds: 120
    # Interval between probe executions.
    periodSeconds: 20
    tcpSocket:
      # Port checked to determine liveness.
      port: http
  # Defines the restart policy for TBMQ-IE pods.
  restartPolicy: Always
  securityContext:
    # User ID to run the container.
    runAsUser: 799
    # Enforces non-root execution.
    runAsNonRoot: true
    # File system group ID for permissions.
    fsGroup: 799
  # Defines CPU/memory requests & limits.
  # Example:
  # resources:
  #   limits:
  #     cpu: "1.5"
  #     memory: 3Gi
  #   # Default requests for CPU and Memory that pods will try to use
  #   requests:
  #     cpu: "1.5"
  #     memory: 3Gi
  # See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#pod-level-resource-specification for more details.
  resources: {}

# This section will bring bitnami/redis-cluster (https://artifacthub.io/packages/helm/bitnami/redis-cluster) into this chart.
# If you want to add some extra configuration parameters, you can put them under the `redis-cluster` key, and they will be passed to bitnami/redis-cluster chart.
redis-cluster:
  ## @param nameOverride String to partially override common.names.fullname template (will maintain the release name)
  ##
  nameOverride: "redis"
  ## @param password Redis&reg; password (ignored if existingSecret set)
  ## Defaults to a random 10-character alphanumeric string if not set and usePassword is true
  ## ref: https://github.com/bitnami/containers/tree/main/bitnami/redis#setting-the-server-password-on-first-run
  ##
  password: "myredispassword"
  ## @param existingSecret Name of existing secret object (for password authentication)
  ##
  existingSecret: ""
  ## @param existingSecretPasswordKey Name of key containing password to be retrieved from the existing secret
  ##
  existingSecretPasswordKey: ""
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  ##
  pdb:
    ## @param pdb.create Created a PodDisruptionBudget
    ##
    create: false
    ## @param pdb.maxUnavailable Max number of pods that can be unavailable after the eviction.
    ## You can specify an integer or a percentage by setting the value to a string representation of a percentage (e.g. "50%"). It will be disabled if set to 0
    ##
    maxUnavailable: 1
  ## @section Redis&reg; statefulset parameters
  ##
  redis:
    enabled: true
    ## @param redis.useAOFPersistence Whether to use AOF Persistence mode or not
    ## It is strongly recommended to use this type when dealing with clusters
    ## ref: https://redis.io/topics/persistence#append-only-file
    ## ref: https://redis.io/topics/cluster-tutorial#creating-and-using-a-redis-cluster
    ##
    useAOFPersistence: "yes"
    ## @param redis.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## @param redis.podAntiAffinityPreset Redis&reg; pod anti-affinity preset. Ignored if `redis.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Redis&reg; node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param redis.nodeAffinityPreset.type Redis&reg; node affinity preset type. Ignored if `redis.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param redis.nodeAffinityPreset.key Redis&reg; node label key to match Ignored if `redis.affinity` is set.
      ## E.g.
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ""
      ## @param redis.nodeAffinityPreset.values Redis&reg; node label values to match. Ignored if `redis.affinity` is set.
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    affinity: {}
    ## @param redis.nodeSelector Node labels for Redis&reg; pods assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    ## @param persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size Size of data volume
    ##
    size: 8Gi
  ## persistentVolumeClaimRetentionPolicy
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention
  ## @param persistentVolumeClaimRetentionPolicy.enabled Controls if and how PVCs are deleted during the lifecycle of a StatefulSet
  ## @param persistentVolumeClaimRetentionPolicy.whenScaled Volume retention behavior when the replica count of the StatefulSet is reduced
  ## @param persistentVolumeClaimRetentionPolicy.whenDeleted Volume retention behavior that applies when the StatefulSet is deleted
  persistentVolumeClaimRetentionPolicy:
    enabled: false
    whenScaled: Retain
    whenDeleted: Retain
  ## Redis&reg; Cluster settings
  ##
  cluster:
    ## Number of Redis&reg; nodes to be deployed
    ##
    ## Note:
    ## This is total number of nodes including the replicas. Meaning there will be 3 master and 3 replica
    ## nodes (as replica count is set to 1 by default, there will be 1 replica per master node).
    ## Hence, nodes = numberOfMasterNodes + numberOfMasterNodes * replicas
    ##
    ## @param cluster.nodes The number of master nodes should always be >= 3, otherwise cluster creation will fail
    ##
    nodes: 6
    ## @param cluster.replicas Number of replicas for every master in the cluster
    ## Parameter to be passed as --cluster-replicas to the redis-cli --cluster create
    ## 1 means that we want a replica for every master created
    ##
    replicas: 1
    ## This section allows to update the Redis&reg; cluster nodes.
    ##
    update:
      ## @param cluster.update.addNodes Boolean to specify if you want to add nodes after the upgrade
      ## Setting this to true a hook will add nodes to the Redis&reg; cluster after the upgrade. currentNumberOfNodes and currentNumberOfReplicas is required
      ##
      addNodes: false
      ## @param cluster.update.currentNumberOfNodes Number of currently deployed Redis&reg; nodes
      ##
      currentNumberOfNodes: 6
      ## @param cluster.update.currentNumberOfReplicas Number of currently deployed Redis&reg; replicas
      ##
      currentNumberOfReplicas: 1
      ## @param cluster.update.newExternalIPs External IPs obtained from the services for the new nodes to add to the cluster
      ##
      newExternalIPs: []
  ## Prometheus Exporter / Metrics
  ##
  metrics:
    ## @param metrics.enabled Start a side-car prometheus exporter
    ##
    enabled: false
    ## @param metrics.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ##
    resources: {}

# If you're deploying PostgreSQL externally, configure this section
externalPostgresql:
  # @param host - External PostgreSQL server host
  host: ""
  # @param port - External PostgreSQL server port
  ##
  port: 5432
  # @param password - PostgreSQL user
  ##
  username: "postgres"
  # @param password - PostgreSQL user password
  ##
  password: "postgres"
  # @param database - PostgreSQL database name for TBMQ
  ##
  database: "thingsboard_mqtt_broker"

# This section will bring bitnami/postgresql (https://artifacthub.io/packages/helm/bitnami/postgresql) into this chart.
#  If you want to add some extra configuration parameters, you can put them under the `postgresql` key, and they will be passed to bitnami/postgresql chart
postgresql:
  # @param enabled If enabled is set to true, externalPostgresql configuration will be ignored
  enabled: true
  ## @param nameOverride String to partially override common.names.fullname template (will maintain the release name)
  ##
  nameOverride: "postgresql"
  ## Authentication parameters
  ## ref: https://github.com/bitnami/containers/tree/main/bitnami/postgresql#setting-the-root-password-on-first-run
  ## ref: https://github.com/bitnami/containers/tree/main/bitnami/postgresql#creating-a-database-on-first-run
  ## ref: https://github.com/bitnami/containers/tree/main/bitnami/postgresql#creating-a-database-user-on-first-run
  ##
  auth:
    ## @param auth.enablePostgresUser Assign a password to the "postgres" admin user. Otherwise, remote access will be blocked for this user
    ##
    enablePostgresUser: true
    ## @param auth.postgresPassword Password for the "postgres" admin user. Ignored if `auth.existingSecret` is provided
    ##
    postgresPassword: ""
    ## @param auth.username Name for a custom user to create
    ##
    username: ""
    ## @param auth.password Password for the custom user to create. Ignored if `auth.existingSecret` is provided
    ##
    password: ""
    ## @param auth.database PostgreSQL database name for TBMQ.
    ##
    database: "thingsboard_mqtt_broker"
    ## @param auth.existingSecret Name of existing secret to use for PostgreSQL credentials. `auth.postgresPassword`, `auth.password`, and `auth.replicationPassword` will be ignored and picked up from this secret. The secret might also contains the key `ldap-password` if LDAP is enabled. `ldap.bind_password` will be ignored and picked from this secret in this case.
    ##
    existingSecret: ""
    ## @param auth.secretKeys.adminPasswordKey Name of key in existing secret to use for PostgreSQL credentials. Only used when `auth.existingSecret` is set.
    ## @param auth.secretKeys.userPasswordKey Name of key in existing secret to use for PostgreSQL credentials. Only used when `auth.existingSecret` is set.
    secretKeys:
      adminPasswordKey: postgres-password
      userPasswordKey: password
  ## @section PostgreSQL Primary parameters
  ##
  primary:
    ## @param primary.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## @param primary.nodeSelector Node labels for PostgreSQL primary pods assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## PostgreSQL Primary persistence configuration
    ##
    persistence:
      ## @param primary.persistence.storageClass PVC Storage Class for PostgreSQL Primary data volume
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      storageClass: ""
      ## @param primary.persistence.accessModes PVC Access Mode for PostgreSQL volume
      ##
      accessModes:
        - ReadWriteOnce
      ## @param primary.persistence.size PVC Storage Request for PostgreSQL volume
      ##
      size: 8Gi
  ## @section Backup parameters
  ## This section implements a trivial logical dump cronjob of the database.
  ## This only comes with the consistency guarantees of the dump program.
  ## This is not a snapshot based roll forward/backward recovery backup.
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/
  backup:
    ## @param backup.enabled Enable the logical dump of the database "regularly"
    enabled: false
    cronjob:
      ## @param backup.cronjob.schedule Set the cronjob parameter schedule
      schedule: "@daily"
      ## @param backup.cronjob.nodeSelector Node labels for PostgreSQL backup CronJob pod assignment
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
      ##
      nodeSelector: {}
      ## @param backup.cronjob.resources Set container requests and limits for different resources like CPU or memory
      ## Example:
      resources: {}
      storage:
        ## @param backup.cronjob.storage.size PVC Storage Request for the backup data volume
        ##
        size: 8Gi
  ## @section Metrics Parameters
  ##
  metrics:
    ## @param metrics.enabled Start a prometheus exporter
    ##
    enabled: false
    ## @param metrics.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Service configuration
    ##
    service:
      ## @param metrics.service.ports.metrics PostgreSQL Prometheus Exporter service port
      ##
      ports:
        metrics: 9187

  # Name of an existing ConfigMap that will override Postgres configurations.
  existingConfigMap: ""
# This section will bring bitnami/kafka (https://artifacthub.io/packages/helm/bitnami/kafka) into this chart.
# If you want to add some extra configuration parameters, you can put them under the `kafka` key, and they will be passed to bitnami/kafka chart
kafka:
  enabled: true
  # Default name override for bitnami/kafka
  nameOverride: "kafka"
  ## @param extraConfig Additional configuration to be appended at the end of the generated Kafka configuration file.
  ##
  extraConfig: |
    auto.create.topics.enable=false
    default.replication.factor=2
    offsets.topic.replication.factor=3
    transaction.state.log.replication.factor=3
    transaction.state.log.min.isr=2
  ## @param heapOpts Kafka Java Heap size
  ##
  heapOpts: -Xmx1024m -Xms1024m
  ## Kafka listeners configuration
  ##
  listeners:
    ## @param listeners.client.name Name for the Kafka client listener
    ## @param listeners.client.containerPort Port for the Kafka client listener
    ## @param listeners.client.protocol Security protocol for the Kafka client listener. Allowed values are 'PLAINTEXT', 'SASL_PLAINTEXT', 'SASL_SSL' and 'SSL'
    ## @param listeners.client.sslClientAuth Optional. If SASL_SSL is enabled, configure mTLS TLS authentication type. If SSL protocol is enabled, overrides tls.authType for this listener. Allowed values are 'none', 'requested' and 'required'
    client:
      protocol: PLAINTEXT
    ## @param listeners.controller.name Name for the Kafka controller listener
    ## @param listeners.controller.containerPort Port for the Kafka controller listener
    ## @param listeners.controller.protocol Security protocol for the Kafka controller listener. Allowed values are 'PLAINTEXT', 'SASL_PLAINTEXT', 'SASL_SSL' and 'SSL'
    ## @param listeners.controller.sslClientAuth Optional. If SASL_SSL is enabled, configure mTLS TLS authentication type. If SSL protocol is enabled, overrides tls.authType for this listener. Allowed values are 'none', 'requested' and 'required'
    ## Ref: https://cwiki.apache.org/confluence/display/KAFKA/KIP-684+-+Support+mutual+TLS+authentication+on+SASL_SSL+listeners
    controller:
      protocol: PLAINTEXT
    ## @param listeners.interbroker.name Name for the Kafka inter-broker listener
    ## @param listeners.interbroker.containerPort Port for the Kafka inter-broker listener
    ## @param listeners.interbroker.protocol Security protocol for the Kafka inter-broker listener. Allowed values are 'PLAINTEXT', 'SASL_PLAINTEXT', 'SASL_SSL' and 'SSL'
    ## @param listeners.interbroker.sslClientAuth Optional. If SASL_SSL is enabled, configure mTLS TLS authentication type. If SSL protocol is enabled, overrides tls.authType for this listener. Allowed values are 'none', 'requested' and 'required'
    interbroker:
      protocol: PLAINTEXT
    ## @param listeners.external.containerPort Port for the Kafka external listener
    ## @param listeners.external.protocol Security protocol for the Kafka external listener. . Allowed values are 'PLAINTEXT', 'SASL_PLAINTEXT', 'SASL_SSL' and 'SSL'
    ## @param listeners.external.name Name for the Kafka external listener
    ## @param listeners.external.sslClientAuth Optional. If SASL_SSL is enabled, configure mTLS TLS authentication type. If SSL protocol is enabled, overrides tls.sslClientAuth for this listener. Allowed values are 'none', 'requested' and 'required'
    external:
      protocol: PLAINTEXT
  ## @section Controller-eligible statefulset parameters
  ##
  controller:
    ## @param controller.replicaCount Number of Kafka controller-eligible nodes
    ## Ignore this section if running in Zookeeper mode.
    ##
    replicaCount: 3
    ## @param controller.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## @param controller.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Node affinity preset
    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param controller.nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param controller.nodeAffinityPreset.key Node label key to match Ignored if `affinity` is set.
      ## E.g.
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ""
      ## @param controller.nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set.
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param controller.affinity Affinity for pod assignment
    ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param controller.nodeSelector Node labels for pod assignment
    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## Kafka Pod Disruption Budget
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
    ## @param controller.pdb.create Deploy a pdb object for the Kafka pod
    ## @param controller.pdb.minAvailable Maximum number/percentage of unavailable Kafka replicas
    ## @param controller.pdb.maxUnavailable Maximum number/percentage of unavailable Kafka replicas
    ##
    pdb:
      create: false
      maxUnavailable: 1
    ## Enable persistence using Persistent Volume Claims
    ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
    ##
    persistence:
      ## @param controller.persistence.existingClaim A manually managed Persistent Volume and Claim
      ## If defined, PVC must be created manually before volume will be bound
      ## The value is evaluated as a template
      ##
      existingClaim: ""
      ## @param controller.persistence.storageClass PVC Storage Class for Kafka data volume
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ## set, choosing the default provisioner.
      ##
      storageClass: ""
      ## @param controller.persistence.accessModes Persistent Volume Access Modes
      ##
      accessModes:
        - ReadWriteOnce
      ## @param controller.persistence.size PVC Storage Request for Kafka data volume
      ##
      size: 8Gi
  ## Prometheus Exporters / Metrics
  ##
  metrics:
    ## Prometheus JMX exporter: exposes the majority of Kafka metrics
    ##
    jmx:
      ## @param metrics.jmx.enabled Whether or not to expose JMX metrics to Prometheus
      ##
      enabled: false
      ## @param metrics.jmx.kafkaJmxPort JMX port where the exporter will collect metrics, exposed in the Kafka container.
      ##
      kafkaJmxPort: 5555
      ## @param metrics.kafka.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
      ## Example:
      ## resources:
      ##   requests:
      ##     cpu: 2
      ##     memory: 512Mi
      ##   limits:
      ##     cpu: 3
      ##     memory: 1024Mi
      ##
      resources: {}
  # Name of an existing ConfigMap that will override Kafka configurations.
  existingConfigMap: ""
# Load Balancer Configuration
# This section defines the load balancer settings for TBMQ.
# Supported types:
# - AWS: Uses Application Load Balancer (ALB) for HTTP(S) traffic and Network Load Balancer (NLB) for MQTT(S) traffic.
# - Azure: Uses Application Gateway for HTTP(S) traffic and Azure Load Balancer for MQTT(S) traffic.
# - GCP: Uses HTTPS Load Balancer for HTTP(S) traffic and Network Load Balancer (NLB) for MQTT(S) traffic.
# - Nginx: Use basic Kubernetes Ingress for HTTP and a LoadBalancer service for MQTT without cloud-specific integrations.
loadbalancer:
  # Load balancer type. Supported values: "nginx", "aws", "azure", "gcp".
  type: "nginx"
  # HTTP(S) Load Balancer Configuration (Application Layer - L7)
  # This load balancer is used for HTTP/HTTPS traffic, such as REST APIs.
  http:
    # Set to false to disable the HTTP load balancer.
    enabled: true
    # Host for the HTTP load balancer.
    host: ""
    # Annotations for HTTP ingress
    annotations: {}
    ssl:
      # Hosts to be configured for the ingress.
      hosts: {}
      # Secret name with the certificate for the ssl termination.
      secretName: ""
      # Enables HTTPS termination at the load balancer level.
      enabled: false
      # Certificate reference:
      # - AWS: The ACM certificate ARN for ALB.
      # - Azure: The `appgw-ssl-certificate` value in Application Gateway.
      # - GCP: The name of the ManagedCertificate resource.
      # - Nginx: Not implemented.
      certificateRef: ""
      # List of domains for HTTPS traffic.
      # - AWS: Not used in Ingress. Domains are part of the ACM certificate specified in `certificateRef`.
      # - Azure: Not used in Ingress. Domains are managed directly in Application Gateway.
      # - GCP: Required in Ingress. Domains must be listed for GCP ManagedCertificate to issue an SSL certificate.
      # - Nginx: Not implemented.
      domains:
        - www.example.com
      # Static IP address for the GCP HTTP(S) load balancer.
      # Required for GCP. Ignored for other types.
      staticIP: "tbmq-http-lb-address"
  # MQTT Load Balancer Configuration (Transport Layer - L4)
  # This load balancer is used for MQTT traffic (TCP).
  mqtt:
    # Set to false to disable the MQTT load balancer.
    enabled: true
    # Two-Way TLS (Mutual TLS or mTLS) - Handled at the TBMQ application level.
    # This ensures that both the client and the server authenticate each other.
    mutualTls:
      # Set to true to enable mutual TLS (mTLS) authentication. Requires server certificate and private key.
      # TLS Termination (see below) will be ignored if this is enabled.
      enabled: false
      # Required: Use an existing ConfigMap instead or create a new one.
      # The ConfigMap must contain the following keys:
      # - `server.pem`: The server certificate.
      # - `mqttserver_key.pem`: The private key.
      # Example of a valid ConfigMap:
      # ---
      # apiVersion: v1
      # kind: ConfigMap
      # metadata:
      #   name: <config_map_name>
      # data:
      #   server.pem: |-
      #     <Your PEM Certificate Content>
      #   mqttserver_key.pem: |-
      #     <Your PEM Private Key Content>
      # ---
      # To create this ConfigMap, use:
      # kubectl create configmap <config_map_name> \
      #   --from-file=server.pem=YOUR_PEM_FILENAME \
      #   --from-file=mqttserver_key.pem=YOUR_PEM_KEY_FILENAME \
      #   -o yaml --dry-run=client | kubectl apply -f -
      #
      configMapName: "tbmq-node-mqtts-config"
      # Optional: Use an existing Secret to provide the private key password.
      # The Secret must contain a key that matches the value of `privateKeyPasswordSecretKey`.
      # By default, named as `key_password`, but this can be overridden.
      # Example of a valid Secret:
      # ---
      # apiVersion: v1
      # kind: Secret
      # metadata:
      #   name: <secret_name>
      # type: Opaque
      # data:
      #   <privateKeyPasswordSecretKey>: <Base64-encoded password>
      # ---
      # To create this Secret, use:
      # kubectl create secret generic <secret_name> \
      #   --from-literal=<secretKey>="YOUR_KEY_PASSWORD" \
      #   -o yaml --dry-run=client | kubectl apply -f -
      #
      privateKeyPasswordSecret: ""
      # The key inside the Kubernetes Secret <privateKeyPasswordSecret> that contains the value of the private key password.
      privateKeyPasswordSecretKey: "key_password"
    # One-Way TLS Termination (Handled at Load Balancer Level - L4)
    # This option allows the load balancer to terminate TLS and pass decrypted traffic to TBMQ.
    # - AWS: TLS termination is supported via Network Load Balancer (NLB) with an ACM certificate.
    # - Azure: TLS termination is NOT supported at the Azure Load Balancer level.
    # - GCP: TLS termination is NOT supported at the GCP Network Load Balancer level.
    # - Nginx: TLS termination is NOT supported.
    tlsTermination:
      # Set to true to enable TLS termination at the load balancer. Will be ignored if @param mutualTls.enabled is set to true
      enabled: false
      # Certificate reference for TLS termination:
      # - AWS: The ACM certificate ARN for NLB.
      # - Azure: Not applicable (this option will be ignored).
      # - GCP: Not applicable (this option will be ignored).
      # - Nginx: Not applicable (this option will be ignored).
      certificateRef: ""
